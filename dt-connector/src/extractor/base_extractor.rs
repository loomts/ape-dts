use std::sync::{
    atomic::{AtomicBool, Ordering},
    Arc,
};

use anyhow::bail;

use dt_common::{
    config::{config_enums::DbType, config_token_parser::ConfigTokenParser},
    error::Error,
    log_error, log_info, log_warn,
    meta::{ddl_meta::ddl_data::DdlData, dt_queue::DtQueue},
    utils::{sql_util::SqlUtil, time_util::TimeUtil},
};
use dt_common::{
    meta::{
        ddl_meta::ddl_parser::DdlParser,
        dt_data::{DtData, DtItem},
        position::Position,
        row_data::RowData,
    },
    time_filter::TimeFilter,
};

use crate::{data_marker::DataMarker, rdb_router::RdbRouter};

use super::extractor_monitor::ExtractorMonitor;

pub struct BaseExtractor {
    pub buffer: Arc<DtQueue>,
    pub router: RdbRouter,
    pub shut_down: Arc<AtomicBool>,
    pub monitor: ExtractorMonitor,
    pub data_marker: Option<DataMarker>,
}

impl BaseExtractor {
    pub fn is_data_marker_info(&self, db: &str, tb: &str) -> bool {
        if let Some(data_marker) = &self.data_marker {
            return data_marker.is_marker_info_2(db, tb);
        }
        false
    }

    pub async fn push_dt_data(
        &mut self,
        dt_data: DtData,
        position: Position,
    ) -> anyhow::Result<()> {
        if let Some(data_marker) = &mut self.data_marker {
            if dt_data.is_begin() || dt_data.is_commit() {
                data_marker.reset();
            } else if data_marker.reseted {
                if data_marker.is_marker_info(&dt_data) {
                    data_marker.refresh(&dt_data);
                    // after data_marker refreshed, discard the marker data itself
                    return Ok(());
                } else {
                    // the first dml/ddl after the last transaction commit is NOT marker_info,
                    // then current transaction should NOT be filtered by default.
                    // set reseted = false, just to make sure is_marker_info won't be called again
                    // in current transaction
                    data_marker.reseted = false;
                }
            }

            // data from origin node are filtered
            if data_marker.filter {
                return Ok(());
            }
        }

        self.monitor.counters.record_count += 1;
        self.monitor.counters.data_size += dt_data.get_data_size();
        self.monitor.try_flush(false);

        let data_origin_node = if let Some(data_marker) = &mut self.data_marker {
            data_marker.data_origin_node.clone()
        } else {
            String::new()
        };

        let item = DtItem {
            dt_data,
            position,
            data_origin_node,
        };
        self.buffer.push(item).await
    }

    pub async fn push_row(&mut self, row_data: RowData, position: Position) -> anyhow::Result<()> {
        let row_data = self.router.route_row(row_data);
        let dt_data = DtData::Dml { row_data };
        self.push_dt_data(dt_data, position).await
    }

    pub async fn push_ddl(&mut self, ddl_data: DdlData, position: Position) -> anyhow::Result<()> {
        let ddl_data = self.router.route_ddl(ddl_data);
        let dt_data = DtData::Ddl { ddl_data };
        self.push_dt_data(dt_data, position).await
    }

    pub async fn parse_ddl(
        &self,
        db_type: &DbType,
        schema: &str,
        query: &str,
    ) -> anyhow::Result<DdlData> {
        let parser = DdlParser::new(db_type.to_owned());
        let parse_result = parser.parse(query);
        if let Err(err) = parse_result {
            let error = format!("failed to parse ddl, will try ignore it, please execute the ddl manually in target, sql: {}, error: {}", query, err);
            log_error!("{}", error);
            bail! {Error::Unexpected(error)}
        }

        // case 1, execute: use db_1; create table tb_1(id int);
        // binlog query.schema == db_1, schema from DdlParser == None
        // case 2, execute: create table db_1.tb_1(id int);
        // binlog query.schema == empty, schema from DdlParser == db_1
        // case 3, execute: use db_1; create table db_2.tb_1(id int);
        // binlog query.schema == db_1, schema from DdlParser == db_2
        let mut ddl_data = parse_result.unwrap();
        ddl_data.default_db = schema.to_string();
        ddl_data.query = query.to_string();
        Ok(ddl_data)
    }

    pub fn precheck_heartbeat(
        &self,
        heartbeat_interval_secs: u64,
        heartbeat_tb: &str,
        db_type: DbType,
    ) -> Vec<String> {
        log_info!(
            "try starting heartbeat, heartbeat_interval_secs: {}, heartbeat_tb: {}, ",
            heartbeat_interval_secs,
            heartbeat_tb
        );

        if heartbeat_interval_secs == 0 || heartbeat_tb.is_empty() {
            log_warn!("heartbeat disabled, heartbeat_tb is empty");
            return vec![];
        }

        let db_tb =
            ConfigTokenParser::parse(heartbeat_tb, &['.'], &SqlUtil::get_escape_pairs(&db_type));

        if db_tb.len() < 2 {
            log_warn!("heartbeat disabled, heartbeat_tb should be like db.tb or schema.tb");
            return vec![];
        }
        db_tb
    }

    pub fn update_time_filter(time_filter: &mut TimeFilter, timestamp: u32, position: &Position) {
        if !time_filter.started && timestamp >= time_filter.start_timestamp {
            time_filter.started = true;
            log_info!("time filter started, position: {}", position.to_string());
        }

        if !time_filter.ended && timestamp >= time_filter.end_timestamp {
            time_filter.ended = true;
            log_info!("time filter ended, position: {}", position.to_string());
        }
    }

    pub async fn wait_task_finish(&mut self) -> anyhow::Result<()> {
        // wait all data to be transfered
        while !self.buffer.is_empty() {
            TimeUtil::sleep_millis(1).await;
        }

        self.monitor.try_flush(true);
        self.shut_down.store(true, Ordering::Release);
        Ok(())
    }
}
